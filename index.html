<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="keywords" content="function spaces, kernel methods, sparsity, wavelets, neural networks, deep learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>A Function-Space Tour of Data Science</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
  <body>


    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">A Function-Space Tour of Data Science</h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://rahul.sh" target="_blank">Rahul Parhi</a>,</span>
                <span class="author-block">
                  <a href="https://gregongie.github.io/" target="_blank">Greg Ongie</a></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://cpal.cc/" target="_blank">Conference on Parsimony and Learning (CPAL)</a><br> March 2025</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="slides.pdf" target="_blank"
                                         class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Tutorial Slides</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
              Parametric methods aim to explain data with a finite number of
              learnable parameters. These models are typically applied in settings
              where the number of data is greater than the number of parameters.
              Nonparametric methods, on the other hand, model data using
              infinite-dimensional function spaces and/or allow the number of
              parameters to grow beyond the number of data (a.k.a. the
              overparameterized regime). Many classical methods in data science fit
              into this latter framework, including kernel methods and wavelet
              methods. Furthermore, modern methods based on overparameterized
              neural networks also fit into this framework. The common theme being
              that these methods aim to minimize a quantity in function space. This
              tutorial will provide a tour of nonparametric methods in data science
              through the lens of function spaces starting with classical methods
              such as kernel methods (reproducing kernel Hilbert spaces) and
              wavelet methods (bounded variation spaces, Besov spaces) and ending
              with modern, high-dimensional methods such as overparameterized
              neural networks (variation spaces, Barron spaces). Remarkably, all of
              these methods can be viewed through the lens of abstract representer
              theorems (beyond Hilbert spaces). A particular emphasis will be made
              on the difference between &ell;<sup>2</sup>-regularization (kernel
              methods) and sparsity-promoting &ell;<sup>1</sup>-regularization
              (wavelet methods, neural networks) through the concept of adaptivity.
              For each method/function space, topics such as generalization bounds,
              metric entropy, and minimax rates will be covered.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">References</h2>
          <div class="container">
            <div class="columns is-centered">
              <div class="column is-8">
                <div class="content">
                  <p>
                  <div id="refs" class="references csl-bib-body hanging-indent"
                                 data-entry-spacing="0" role="list">
                    <div id="ref-theory-reproducing-kernels" class="csl-entry"
                                                             role="listitem">
                      Aronszajn, Nachman. 1950. <span>“Theory of Reproducing Kernels.”</span>
                      <em>Transactions of the American Mathematical Society</em> 68 (3):
                      337–404.
                    </div>
                    <div id="ref-AroraCPWL" class="csl-entry" role="listitem">
                      Arora, Raman, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. 2018.
                      <span>“Understanding Deep Neural Networks with Rectified Linear
                        Units.”</span> In <em>International Conference on Learning
                        Representations (ICLR)</em>.
                    </div>
                    <div id="ref-arora2019implicit" class="csl-entry" role="listitem">
                      Arora, Sanjeev, Nadav Cohen, Wei Hu, and Yuping Luo. 2019.
                      <span>“Implicit Regularization in Deep Matrix Factorization.”</span>
                      <em>Advances in Neural Information Processing Systems (NeurIPS)</em> 32.
                    </div>
                    <div id="ref-BachConvexNN" class="csl-entry" role="listitem">
                      Bach, Francis. 2017. <span>“Breaking the Curse of Dimensionality with
                        Convex Neural Networks.”</span> <em>Journal of Machine Learning
                        Research</em> 18 (1): 629–81.
                    </div>
                    <div id="ref-BarronApprox" class="csl-entry" role="listitem">
                      Barron, Andrew R. 1993. <span>“Universal Approximation Bounds for
                        Superpositions of a Sigmoidal Function.”</span> <em>IEEE Transactions on
                        Information Theory</em> 39 (3): 930–45.
                    </div>
                    <div id="ref-BartolucciRKBS" class="csl-entry" role="listitem">
                      Bartolucci, Francesca, Ernesto De Vito, Lorenzo Rosasco, and Stefano
                      Vigogna. 2023. <span>“Understanding Neural Networks with Reproducing
                        Kernel <span>B</span>anach Spaces.”</span> <em>Applied and Computational
                        Harmonic Analysis</em> 62: 194–236.
                    </div>
                    <div id="ref-bartolucci2024neural" class="csl-entry" role="listitem">
                      Bartolucci, Francesca, Ernesto De Vito, Lorenzo Rosasco, and Stefano
                      Vigogna. 2024. <span>“Neural Reproducing Kernel <span>B</span>anach Spaces
                        and Representer Theorems for Deep Networks.”</span> <em>arXiv Preprint
                        arXiv:2403.08750</em>.
                    </div>
                    <div id="ref-bietti2022learning" class="csl-entry" role="listitem">
                      Bietti, Alberto, Joan Bruna, Clayton Sanford, and Min Jae Song. 2022.
                      <span>“Learning Single-Index Models with Shallow Neural
                        Networks.”</span> In <em>Advances in Neural Information Processing
                        Systems</em>.
                    </div>
                    <div id="ref-BoyerRepresenter" class="csl-entry" role="listitem">
                      Boyer, Claire, Antonin Chambolle, Yohann De Castro, Vincent Duval,
                      Frédéric De Gournay, and Pierre Weiss. 2019. <span>“On Representer
                        Theorems and Convex Regularization.”</span> <em>SIAM Journal on
                        Optimization</em> 29 (2): 1260–81.
                    </div>
                    <div id="ref-BrediesRepresenter" class="csl-entry" role="listitem">
                      Bredies, Kristian, and Marcello Carioni. 2020. <span>“Sparsity of
                        Solutions for Variational Inverse Problems with Finite-Dimensional
                        Data.”</span> <em>Calculus of Variations and Partial Differential
                        Equations</em> 59 (1): 1–26.
                    </div>
                    <div id="ref-burer2003nonlinear" class="csl-entry" role="listitem">
                      Burer, Samuel, and Renato DC Monteiro. 2003. <span>“A Nonlinear
                        Programming Algorithm for Solving Semidefinite Programs via Low-Rank
                        Factorization.”</span> <em>Mathematical Programming</em> 95 (2): 329–57.
                    </div>
                    <div id="ref-CandesCS" class="csl-entry" role="listitem">
                      Candès, Emmanuel J., Justin Romberg, and Terence Tao. 2006.
                      <span>“Robust Uncertainty Principles: Exact Signal Reconstruction from
                        Highly Incomplete Frequency Information.”</span> <em>IEEE Transactions
                        on Information Theory</em> 52 (2): 489–509.
                    </div>
                    <div id="ref-carl1981entropy" class="csl-entry" role="listitem">
                      Carl, Bernd. 1981. <span>“Entropy Numbers, <span><span
                                                                           class="math inline"><em>s</em></span></span>-Numbers, and Eigenvalue
                                                                       Problems.”</span> <em>Journal of Functional Analysis</em> 41 (3):
                                                                     290–306.
                    </div>
                    <div id="ref-chandrasekaran2012convex" class="csl-entry"
                                                           role="listitem">
                      Chandrasekaran, Venkat, Benjamin Recht, Pablo A. Parrilo, and Alan S.
                      Willsky. 2012. <span>“The Convex Geometry of Linear Inverse
                        Problems.”</span> <em>Foundations of Computational Mathematics</em> 12
                      (6): 805–49.
                    </div>
                    <div id="ref-chen2024neural" class="csl-entry" role="listitem">
                      Chen, Zhengdao. 2024. <span>“Neural <span>H</span>ilbert Ladders:
                        Multi-Layer Neural Networks in Function Space.”</span> <em>Journal of
                        Machine Learning Research</em> 25 (109): 1–65.
                    </div>
                    <div id="ref-dai2021representation" class="csl-entry" role="listitem">
                      Dai, Zhen, Mina Karzand, and Nathan Srebro. 2021. <span>“Representation
                        Costs of Linear Neural Networks: Analysis and Design.”</span>
                      <em>Advances in Neural Information Processing Systems</em> 34: 26884–96.
                    </div>
                    <div id="ref-Damian_Lee_Soltanolkotabi_2022" class="csl-entry"
                                                                 role="listitem">
                      Damian, Alexandru, Jason Lee, and Mahdi Soltanolkotabi. 2022.
                      <span>“Neural Networks Can Learn Representations with Gradient
                        Descent.”</span> In <em>Conference on Learning Theory</em>, 5413–52.
                      PMLR.
                    </div>
                    <div id="ref-devore1998nonlinear" class="csl-entry" role="listitem">
                      DeVore, Ronald A. 1998. <span>“Nonlinear Approximation.”</span> <em>Acta
                        Numerica</em> 7: 51–150.
                    </div>
                    <div id="ref-devore2025weighted" class="csl-entry" role="listitem">
                      DeVore, Ronald, Robert D. Nowak, Rahul Parhi, and Jonathan W. Siegel.
                      2025. <span>“Weighted Variation Spaces and Approximation by Shallow
                        <span>ReLU</span> Networks.”</span> <em>Applied and Computational
                        Harmonic Analysis</em> 74 (101713).
                    </div>
                    <div id="ref-DonohoMixedVariation" class="csl-entry" role="listitem">
                      Donoho, David L. 2000. <span>“High-Dimensional Data Analysis: The Curses
                        and Blessings of Dimensionality.”</span> <em>AMS Math Challenges
                        Lecture</em> 1 (2000): 32.
                    </div>
                    <div id="ref-DonohoCS" class="csl-entry" role="listitem">
                      Donoho, David L.. 2006. <span>“Compressed Sensing.”</span> <em>IEEE Transactions on
                        Information Theory</em> 52 (4): 1289–1306.
                    </div>
                    <div id="ref-donoho1994ideal" class="csl-entry" role="listitem">
                      Donoho, David L., and Iain M. Johnstone. 1994. <span>“Ideal Spatial
                        Adaptation by Wavelet Shrinkage.”</span> <em>Biometrika</em> 81 (3):
                      425–55.
                    </div>
                    <div id="ref-donoho1995adapting" class="csl-entry" role="listitem">
                      Donoho, David L., and Iain M. Johnstone. 1995. <span>“Adapting to Unknown Smoothness via Wavelet
                        Shrinkage.”</span> <em>Journal of the American Statistical
                        Association</em> 90 (432): 1200–1224.
                    </div>
                    <div id="ref-DonohoWaveletShrinkage" class="csl-entry" role="listitem">
                      Donoho, David L., and Iain M. Johnstone. 1998. <span>“Minimax Estimation via Wavelet Shrinkage.”</span>
                      <em>The Annals of Statistics</em> 26 (3): 879–921.
                    </div>
                    <div id="ref-donoho1990minimax" class="csl-entry" role="listitem">
                      Donoho, David L., Richard C. Liu, and Brenda MacGibbon. 1990.
                      <span>“Minimax Risk over Hyperrectangles, and Implications.”</span>
                      <em>Annals of Statistics</em>, 1416–37.
                    </div>
                    <div id="ref-ma2022barron" class="csl-entry" role="listitem">
                      E, Weinan, Chao Ma, and Lei Wu. 2022. <span>“The <span>B</span>arron
                        Space and the Flow-Induced Function Spaces for Neural Network
                        Models.”</span> <em>Constructive Approximation</em> 55 (1): 369–406.
                    </div>
                    <div id="ref-wojtowytsch2020banach" class="csl-entry" role="listitem">
                      E, Weinan, and Stephan Wojtowytsch. 2020. <span>“On the
                        <span>B</span>anach Spaces Associated with Multi-Layer <span>ReLU</span>
                        Networks: <span>F</span>unction Representation, Approximation Theory and
                        Gradient Descent Dynamics.”</span> <em>CSIAM Transactions on Applied
                        Mathematics</em> 1 (3): 387–440.
                    </div>
                    <div id="ref-ergen2021convex" class="csl-entry" role="listitem">
                      Ergen, Tolga, and Mert Pilanci. 2021. <span>“Convex Geometry and Duality
                        of over-Parameterized Neural Networks.”</span> <em>Journal of Machine
                        Learning Research</em>.
                    </div>
                    <div id="ref-FisherJerome" class="csl-entry" role="listitem">
                      Fisher, Stephen D., and Joseph W. Jerome. 1975. <span>“Spline Solutions
                        to <span><span class="math inline"><em>L</em><sup>1</sup></span></span>
                        Extremal Problems in One and Several Variables.”</span> <em>Journal of
                        Approximation Theory</em> 13 (1): 73–83.
                    </div>
                    <div id="ref-vandeGeerBook" class="csl-entry" role="listitem">
                      Geer, Sara van de. 2000. <em>Empirical Processes in
                        <span>M</span>-Estimation</em>. Vol. 6. Cambridge University Press.
                    </div>
                    <div id="ref-golubeva2020wider" class="csl-entry" role="listitem">
                      Golubeva, Anna, Guy Gur-Ari, and Behnam Neyshabur. 2021. <span>“Are
                        Wider Nets Better Given the Same Number of Parameters?”</span> In
                      <em>International Conference on Learning Representations</em>.
                    </div>
                    <div id="ref-grandvalet1998least" class="csl-entry" role="listitem">
                      Grandvalet, Yves. 1998. <span>“Least Absolute Shrinkage Is Equivalent to
                        Quadratic Penalization.”</span> In <em>International Conference on
                        Artificial Neural Networks</em>, 201–6. Springer.
                    </div>
                    <div id="ref-gunasekar2018implicit" class="csl-entry" role="listitem">
                      Gunasekar, Suriya, Jason D Lee, Daniel Soudry, and Nati Srebro. 2018.
                      <span>“Implicit Bias of Gradient Descent on Linear Convolutional
                        Networks.”</span> <em>Advances in Neural Information Processing
                        Systems</em> 31.
                    </div>
                    <div id="ref-heeringa2025deep" class="csl-entry" role="listitem">
                      Heeringa, Tjeerd Jan, Len Spek, and Christoph Brune. 2025. <span>“Deep
                        Networks Are Reproducing Kernel Chains.”</span> <em>arXiv Preprint
                        arXiv:2501.03697</em>.
                    </div>
                    <div id="ref-JacotBNRank" class="csl-entry" role="listitem">
                      Jacot, Arthur. 2023a. <span>“Implicit Bias of Large Depth Networks: A
                        Notion of Rank for Nonlinear Functions.”</span> In <em>International
                        Conference on Learning Representations (ICLR)</em>.
                    </div>
                    <div id="ref-JacotBNRegularity" class="csl-entry" role="listitem">
                      Jacot, Arthur. 2023b. <span>“Bottleneck Structure in Learned Features:
                        Low-Dimension Vs Regularity Tradeoff.”</span> <em>Advances in Neural
                        Information Processing Systems</em> 36 (December): 23607–29.
                    </div>
                    <div id="ref-JacotNTK" class="csl-entry" role="listitem">
                      Jacot, Arthur, Franck Gabriel, and Clement Hongler. 2018. <span>“Neural
                        Tangent Kernel: Convergence and Generalization in Neural
                        Networks.”</span> In <em>Advances in Neural Information Processing
                        Systems</em>. Vol. 31.
                    </div>
                    <div id="ref-JacotFeature" class="csl-entry" role="listitem">
                      Jacot, Arthur, Eugene Golikov, Clément Hongler, and Franck Gabriel.
                      2022. <span>“Feature Learning in <span><span
                                                                 class="math inline"><em>L</em><sub>2</sub></span></span>-Regularized
                                                             <span>DNN</span>s: Attraction/Repulsion and Sparsity.”</span>
                      <em>Advances in Neural Information Processing Systems</em> 35: 6763–74.
                    </div>
                    <div id="ref-JacotHamiltonian" class="csl-entry" role="listitem">
                      Jacot, Arthur, and Alexandre Kaiser. 2025. <span>“Hamiltonian Mechanics
                        of Feature Learning: Bottleneck Structure in Leaky
                        <span>R</span>es<span>N</span>ets.”</span> <em>Conference on Parsimony
                        and Learning (CPAL)</em>.
                    </div>
                    <div id="ref-JacotNC" class="csl-entry" role="listitem">
                      Jacot, Arthur, Peter Súkenı́k, Zihan Wang, and Marco Mondelli. 2024.
                      <span>“Wide Neural Networks Trained with Weight Decay Provably Exhibit
                        Neural Collapse.”</span> <em>arXiv Preprint arXiv:2410.04887</em>.
                    </div>
                    <div id="ref-klusowski2018approximation" class="csl-entry"
                                                             role="listitem">
                      Klusowski, Jason M, and Andrew R Barron. 2018. <span>“Approximation by
                        Combinations of <span>ReLU</span> and Squared <span>ReLU</span> Ridge
                        Functions with <span><span
                                                 class="math inline">ℓ<sup>1</sup></span></span> and <span><span
                                                 class="math inline">ℓ<sup>0</sup></span></span> Controls.”</span>
                      <em>IEEE Transactions on Information Theory</em> 64 (12): 7649–56.
                    </div>
                    <div id="ref-kurkova2001bounds" class="csl-entry" role="listitem">
                      Kůrková, Věra, and Marcello Sanguineti. 2001. <span>“Bounds on Rates of
                        Variable-Basis and Neural-Network Approximation.”</span> <em>IEEE
                        Transactions on Information Theory</em> 47 (6): 2659–65.
                    </div>
                    <div id="ref-li1991sliced" class="csl-entry" role="listitem">
                      Li, Ker-Chau. 1991. <span>“Sliced Inverse Regression for Dimension
                        Reduction.”</span> <em>Journal of the American Statistical
                        Association</em> 86 (414): 316–27.
                    </div>
                    <div id="ref-lin2022reproducing" class="csl-entry" role="listitem">
                      Lin, Rong Rong, Hai Zhang Zhang, and Jun Zhang. 2022. <span>“On
                        Reproducing Kernel <span>B</span>anach Spaces: <span>G</span>eneric
                        Definitions and Unified Framework of Constructions.”</span> <em>Acta
                        Mathematica Sinica, English Series</em> 38 (8): 1459–83.
                    </div>
                    <div id="ref-MammenLAS" class="csl-entry" role="listitem">
                      Mammen, Enno, and Sara van de Geer. 1997. <span>“Locally Adaptive
                        Regression Splines.”</span> <em>Annals of Statistics</em> 25 (1):
                      387–413.
                    </div>
                    <div id="ref-MatouvsekZonotope" class="csl-entry" role="listitem">
                      Matoušek, Jiří. 1996. <span>“Improved Upper Bounds for Approximation by
                        Zonotopes.”</span> <em>Acta Mathematica</em> 177 (1): 55–73.
                    </div>
                    <div id="ref-mccarty2023piecewise" class="csl-entry" role="listitem">
                      McCarty, Sarah. 2023. <span>“Piecewise Linear Functions Representable
                        with Infinite Width Shallow <span>ReLU</span> Neural Networks.”</span>
                      <em>Proceedings of the American Mathematical Society, Series B</em> 10
                      (27): 296–310.
                    </div>
                    <div id="ref-meyer1992wavelets" class="csl-entry" role="listitem">
                      Meyer, Yves. 1992. <em>Wavelets and Operators</em>. 37. Cambridge
                      University Oress.
                    </div>
                    <div id="ref-mhaskar2004tractability" class="csl-entry" role="listitem">
                      Mhaskar, Hrushikesh N. 2004. <span>“On the Tractability of Multivariate
                        Integration and Approximation by Neural Networks.”</span> <em>Journal of
                        Complexity</em> 20 (4): 561–90.
                    </div>
                    <div id="ref-mousavi2022neural" class="csl-entry" role="listitem">
                      Mousavi-Hosseini, Alireza, Sejun Park, Manuela Girotti, Ioannis
                      Mitliagkas, and Murat A Erdogdu. 2022. <span>“Neural Networks
                        Efficiently Learn Low-Dimensional Representations with
                        <span>SGD</span>.”</span> In <em>The Eleventh International Conference
                        on Learning Representations</em>.
                    </div>
                    <div id="ref-nichani2023provable" class="csl-entry" role="listitem">
                      Nichani, Eshaan, Alex Damian, and Jason D Lee. 2023. <span>“Provable
                        Guarantees for Nonlinear Feature Learning in Three-Layer Neural
                        Networks.”</span> <em>Advances in Neural Information Processing
                        Systems</em> 36: 10828–75.
                    </div>
                    <div id="ref-OngieFunctionSpace" class="csl-entry" role="listitem">
                      Ongie, Greg, Rebecca Willett, Daniel Soudry, and Nathan Srebro. 2020.
                      <span>“A Function Space View of Bounded Norm Infinite Width
                        <span>ReLU</span> Nets: The Multivariate Case.”</span> In
                      <em>International Conference on Learning Representations</em>.
                    </div>
                    <div id="ref-parhi2021banach" class="csl-entry" role="listitem">
                      Parhi, Rahul, and Robert D. Nowak. 2021. <span>“Banach Space Representer
                        Theorems for Neural Networks and Ridge Splines.”</span> <em>Journal of
                        Machine Learning Research</em> 22 (43): 1–40.
                    </div>
                    <div id="ref-parhi2022kinds" class="csl-entry" role="listitem">
                      Parhi, Rahul, and Robert D. Nowak. 2022. <span>“What Kinds of Functions Do Deep Neural Networks Learn?
                        <span>I</span>nsights from Variational Spline Theory.”</span> <em>SIAM
                        Journal on Mathematics of Data Science</em> 4 (2): 464–89.
                    </div>
                    <div id="ref-parhi2022near" class="csl-entry" role="listitem">
                      Parhi, Rahul, and Robert D. Nowak. 2023. <span>“Near-Minimax Optimal Estimation with Shallow
                        <span>ReLU</span> Neural Networks.”</span> <em>IEEE Transactions on
                        Information Theory</em> 69 (2): 1125–40.
                    </div>
                    <div id="ref-parkinson2023relu" class="csl-entry" role="listitem">
                      Parkinson, Suzanna, Greg Ongie, and Rebecca Willett. 2023.
                      <span>“<span>ReLU</span> Neural Networks with Linear Layers Are Biased
                        Towards Single-and Multi-Index Models.”</span> <em>arXiv Preprint
                        arXiv:2305.15598</em>.
                    </div>
                    <div id="ref-petrushev1988direct" class="csl-entry" role="listitem">
                      Petrushev, Pencho P. 1988. <span>“Direct and Converse Theorems for
                        Spline and Rational Approximation and <span>B</span>esov Spaces.”</span>
                      In <em>Function Spaces and Applications: Proceedings of the US-Swedish
                        Seminar Held in Lund, Sweden, June 15–21, 1986</em>, 363–77. Springer.
                    </div>
                    <div id="ref-radhakrishnan2024mechanism" class="csl-entry"
                                                             role="listitem">
                      Radhakrishnan, Adityanarayanan, Daniel Beaglehole, Parthe Pandit, and
                      Mikhail Belkin. 2024. <span>“Mechanism for Feature Learning in Neural
                        Networks and Backpropagation-Free Machine Learning Models.”</span>
                      <em>Science</em> 383 (6690): 1461–67.
                    </div>
                    <div id="ref-razin2020implicit" class="csl-entry" role="listitem">
                      Razin, Noam, and Nadav Cohen. 2020. <span>“Implicit Regularization in
                        Deep Learning May Not Be Explainable by Norms.”</span> <em>Advances in
                        Neural Information Processing Systems</em> 33: 21174–87.
                    </div>
                    <div id="ref-razin2021implicit" class="csl-entry" role="listitem">
                      Razin, Noam, Asaf Maman, and Nadav Cohen. 2021. <span>“Implicit
                        Regularization in Tensor Factorization.”</span> In <em>International
                        Conference on Machine Learning (ICML)</em>, 8913–24.
                    </div>
                    <div id="ref-razin2022implicit" class="csl-entry" role="listitem">
                      Razin, Noam, Asaf Maman, and Nadav Cohen. 2022. <span>“Implicit Regularization in Hierarchical Tensor
                        Factorization and Deep Convolutional Neural Networks.”</span> In
                      <em>International Conference on Machine Learning</em>, 18422–62. PMLR.
                    </div>
                    <div id="ref-razin2023ability" class="csl-entry" role="listitem">
                      Razin, Noam, Tom Verbin, and Nadav Cohen. 2023. <span>“On the Ability of
                        Graph Neural Networks to Model Interactions Between Vertices.”</span>
                      <em>Advances in Neural Information Processing Systems</em> 36: 26501–45.
                    </div>
                    <div id="ref-sahiner2021vectoroutput" class="csl-entry" role="listitem">
                      Sahiner, Arda, Tolga Ergen, John M. Pauly, and Mert Pilanci. 2021.
                      <span>“Vector-Output <span>ReLU</span> Neural Network Problems Are
                        Copositive Programs: Convex Analysis of Two Layer Networks and
                        Polynomial-Time Algorithms.”</span> In <em>International Conference on
                        Learning Representations</em>.
                    </div>
                    <div id="ref-SavareseInfWidth" class="csl-entry" role="listitem">
                      Savarese, Pedro, Itay Evron, Daniel Soudry, and Nathan Srebro. 2019.
                      <span>“How Do Infinite Width Bounded Norm Networks Look in Function
                        Space?”</span> In <em>Conference on Learning Theory (COLT)</em>,
                      2667–90. PMLR.
                    </div>
                    <div id="ref-schmidt2020nonparametric" class="csl-entry"
                                                           role="listitem">
                      Schmidt-Hieber, Johannes. 2020. <span>“Nonparametric Regression Using
                        Deep Neural Networks with <span>ReLU</span> Activation Function.”</span>
                      <em>Annals of Statistics</em> 48 (4): 1875–97.
                    </div>
                    <div id="ref-ScholkopfKernels" class="csl-entry" role="listitem">
                      Schölkopf, Bernhard, and Alexander J. Smola. 2002. <em>Learning with
                        Kernels: Support Vector Machines, Regularization, Optimization, and
                        Beyond</em>. Adaptive Computation and Machine Learning. MIT Press.
                    </div>
                    <div id="ref-shenouda2024variation" class="csl-entry" role="listitem">
                      Shenouda, Joseph, Rahul Parhi, Kangwook Lee, and Robert D Nowak. 2024.
                      <span>“Variation Spaces for Multi-Output Neural Networks: Insights on
                        Multi-Task Learning and Network Compression.”</span> <em>Journal of
                        Machine Learning Research</em> 25 (231): 1–40.
                    </div>
                    <div id="ref-siegel2023optimal" class="csl-entry" role="listitem">
                      Siegel, Jonathan W. 2023. <span>“Optimal Approximation of Zonoids and
                        Uniform Approximation by Shallow Neural Networks.”</span> <em>arXiv
                        Preprint arXiv:2307.15285</em>.
                    </div>
                    <div id="ref-siegel2020approximation" class="csl-entry" role="listitem">
                      Siegel, Jonathan W, and Jinchao Xu. 2020. <span>“Approximation Rates for
                        Neural Networks with General Activation Functions.”</span> <em>Neural
                        Networks</em> 128: 313–21.
                    </div>
                    <div id="ref-siegel2023characterization" class="csl-entry"
                                                             role="listitem">
                      Siegel, Jonathan W, and Jinchao Xu. 2023. <span>“Characterization of the Variation Spaces Corresponding
                        to Shallow Neural Networks.”</span> <em>Constructive Approximation</em>
                      57 (3): 1109–32.
                    </div>
                    <div id="ref-SrebroMF" class="csl-entry" role="listitem">
                      Srebro, Nathan, Jason Rennie, and Tommi Jaakkola. 2004.
                      <span>“Maximum-Margin Matrix Factorization.”</span> <em>Advances in
                        Neural Information Processing Systems</em> 17.
                    </div>
                    <div id="ref-tibshirani1996regression" class="csl-entry"
                                                           role="listitem">
                      Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via
                        the Lasso.”</span> <em>Journal of the Royal Statistical Society Series
                        B: Statistical Methodology</em> 58 (1): 267–88.
                    </div>
                    <div id="ref-timor2023implicit" class="csl-entry" role="listitem">
                      Timor, Nadav, Gal Vardi, and Ohad Shamir. 2023. <span>“Implicit
                        Regularization Towards Rank Minimization in <span>ReLU</span>
                        Networks.”</span> In <em>International Conference on Algorithmic
                        Learning Theory</em>, 1429–59. PMLR.
                    </div>
                    <div id="ref-UnserUnifyingRepresenter" class="csl-entry"
                                                           role="listitem">
                      Unser, Michael. 2021. <span>“A Unifying Representer Theorem for Inverse
                        Problems and Machine Learning.”</span> <em>Foundations of Computational
                        Mathematics</em> 21 (4): 941–60.
                    </div>
                    <div id="ref-unser2023ridges" class="csl-entry" role="listitem">
                      Unser, Michael. 2023. <span>“Ridges, Neural Networks, and the <span>R</span>adon
                        Transform.”</span> <em>Journal of Machine Learning Research</em> 24
                      (37): 1–33.
                    </div>
                    <div id="ref-varshney2024convex" class="csl-entry" role="listitem">
                      Varshney, Prateek, and Mert Pilanci. 2024. <span>“Convex Distillation:
                        Efficient Compression of Deep Networks via Convex Optimization.”</span>
                      <em>arXiv Preprint arXiv:2410.06567</em>.
                    </div>
                    <div id="ref-WahbaSplineModels" class="csl-entry" role="listitem">
                      Wahba, Grace. 1990. <em>Spline Models for Observational Data</em>. Vol.
                      59. SIAM.
                    </div>
                    <div id="ref-MertParallel" class="csl-entry" role="listitem">
                      Wang, Yifei, Tolga Ergen, and Mert Pilanci. 2023. <span>“Parallel Deep
                        Neural Networks Have Zero Duality Gap.”</span> In <em>International
                        Conference on Learning Representations</em>.
                    </div>
                    <div id="ref-JacotCNNs" class="csl-entry" role="listitem">
                      Wen, Yuxiao, and Arthur Jacot. 2024. <span>“Which Frequencies Do
                        <span>CNN</span>s Need? Emergent Bottleneck Structure in Feature
                        Learning.”</span> In <em>International Conference on Machine Learning
                        (ICML)</em>.
                    </div>
                    <div id="ref-wilson2016deep" class="csl-entry" role="listitem">
                      Wilson, Andrew Gordon, Zhiting Hu, Ruslan Salakhutdinov, and Eric P
                      Xing. 2016. <span>“Deep Kernel Learning.”</span> In <em>Artificial
                        Intelligence and Statistics</em>, 370–78. PMLR.
                    </div>
                    <div id="ref-yang2022better" class="csl-entry" role="listitem">
                      Yang, Liu, Jifan Zhang, Joseph Shenouda, Dimitris Papailiopoulos,
                      Kangwook Lee, and Robert D Nowak. 2022. <span>“A Better Way to Decay:
                        Proximal Gradient Training Algorithms for Neural Nets.”</span> In
                      <em>OPT 2022: Optimization for Machine Learning (NeurIPS 2022
                        Workshop)</em>.
                    </div>
                    <div id="ref-zeno2023minimum" class="csl-entry" role="listitem">
                      Zeno, Chen, Greg Ongie, Yaniv Blumenfeld, Nir Weinberger, and Daniel
                      Soudry. 2023. <span>“How Do Minimum-Norm Shallow Denoisers Look in
                        Function Space?”</span> <em>Advances in Neural Information Processing
                        Systems</em> 36: 57520–57.
                    </div>
                    <div id="ref-zhang2009reproducing" class="csl-entry" role="listitem">
                      Zhang, Haizhang, Yuesheng Xu, and Jun Zhang. 2009. <span>“Reproducing
                        Kernel <span>B</span>anach Spaces for Machine Learning.”</span>
                      <em>Journal of Machine Learning Research</em> 10 (12).
                    </div>
                    <div id="ref-zhang2023deep" class="csl-entry" role="listitem">
                      Zhang, Kaiqi, and Yu-Xiang Wang. 2023. <span>“Deep Learning Meets
                        Nonparametric Regression: Are Weight-Decayed <span>DNN</span>s Locally
                        Adaptive?”</span> In <em>International Conference on Learning
                        Representations</em>.
                    </div>
                  </div>

                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Related Links</h2>
          <div class="container">
            <div class="columns is-centered">
              <div class="column is-8">
                <div class="content">
                  <ul>
                    <li><b>Textbook:</b> <a href="https://mjt.cs.illinois.edu/dlt/two.pdf" target="_blank"> Learning Theory (pdf)</a>, Matus Telgarsky, 2023.</li>
                    <li><b>Textbook:</b> <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank">Learning Theory From First Principles (pdf)</a>, Francis Bach, 2024.</li>
                    <li><b>Textbook:</b> <a href="https://mathematical-tours.github.io/book/" target="_blank">Mathematical Foundations of Data Sciences</a>, Gabriel Peyré, 2024.</li>
                    <li><b>Tutorial:</b> <a href="https://www.cis.jhu.edu/~rvidal/talks/learning/Tutorial-Math-Deep-Learning-2018.pdf" target="_blank">Math of Deep Learning (pdf)</a>, Rene Vidal, 2018.</li>
                    <li><b>Blog Post:</b> <a href="https://francisbach.com/quest-for-adaptivity/" target="_blank">The Quest for Adaptivity</a>, Francis Bach, 2023.</li>
                    </ul>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
